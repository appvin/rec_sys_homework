{
 "metadata": {
  "name": "",
  "signature": "sha256:577b1f123cc1b30dad93acccd88ac3b2a22c7bf40c946b5ca1eadcd6903eab2a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import random\n",
      "import pandas as pd\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from scipy.sparse import csr_matrix\n",
      "import logging\n",
      "from gensim import corpora, models, similarities\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.lancaster import LancasterStemmer\n",
      "\n",
      "if __name__ == \"__main__\": \n",
      "    #\u8bfb\u53d6\u6587\u4ef6\n",
      "    items = pd.read_table('u.item', header=None,sep='|')\n",
      "    item_topic=items[1].str.partition('(')[0]\n",
      "    \n",
      "    #\u5c0f\u5199\u5316\uff0c\u53bb\u505c\u7528\u8bcd\uff0c\u53bb\u6807\u70b9\u7b26\u53f7\n",
      "    texts_tokenized = [[word.lower() for word in word_tokenize(document)] for document in item_topic]  \n",
      "    english_stopwords = stopwords.words('english')\n",
      "    texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]\n",
      "    english_punctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
      "    texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords]\n",
      "    \n",
      "    #\u8bcd\u5e72\u5316\n",
      "    st = LancasterStemmer()\n",
      "    texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]\n",
      "    \n",
      "    #\u53bb\u6389\u8bed\u6599\u5e93\u4e2d\u7684\u4f4e\u9891\u8bcd\n",
      "    '''all_stems = sum(texts_stemmed)\n",
      "    stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
      "    texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]'''\n",
      "    texts=texts_stemmed\n",
      "    \n",
      "    #gensim\u9884\u5904\u7406\n",
      "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    tfidf = models.TfidfModel(corpus)\n",
      "    corpus_tfidf = tfidf[corpus]\n",
      "    \n",
      "    #\u8bad\u7ec3topic\u4e3a10\u7684LSI\u6a21\u578b\uff0c\u5efa\u7acb\u7d22\u5f15\n",
      "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) \n",
      "    index = similarities.MatrixSimilarity(lsi[corpus],num_features=10)\n",
      "    \n",
      "    #\u67e5\u8be2\u4e3e\u4f8b210,\u67e5\u8be2\u524d10\u4e2a\u6700\u76f8\u4f3c\u7684\n",
      "    ml_course = ['story']\n",
      "    ml_bow = dictionary.doc2bow(ml_course)\n",
      "    ml_lsi = lsi[ml_bow]\n",
      "    sims = index[ml_lsi]\n",
      "    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
      "    print(sort_sims[0:10])\n",
      "    for i in sort_sims[0:10]:\n",
      "        print(item_topic[i[0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.99911708), (1343, 0.99902445), (598, 0.99762046), (477, 0.98656458), (307, 0.96134549), (1394, 0.91310889), (1606, 0.91310889), (513, 0.90581596), (695, 0.88109541), (1652, 0.87488347)]\n",
        "Toy Story \n",
        "Story of Xinghua, The \n",
        "Police Story 4: Project S \n",
        "Philadelphia Story, The \n",
        "FairyTale: A True Story \n",
        "Hurricane Streets \n",
        "Hurricane Streets \n",
        "Annie Hall \n",
        "City Hall \n",
        "Entertaining Angels: The Dorothy Day Story \n"
       ]
      }
     ],
     "prompt_number": 22
    }
   ],
   "metadata": {}
  }
 ]
}